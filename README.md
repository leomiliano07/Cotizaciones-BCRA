# Cotizaciones frente al peso argentino ü™ô
Este proyecto ETL permite extraer datos de cotizaciones cambiarias desde la API del Banco Central de la Rep√∫blica Argentina (BCRA), procesar la informaci√≥n y cargarla en una base de datos Amazon Redshift utilizando Apache Airflow desplegado en Docker.

## Descripci√≥n

El objetivo de este proyecto es automatizar el proceso de obtenci√≥n y almacenamiento de cotizaciones cambiarias en una base de datos para su posterior an√°lisis y consulta. El flujo del proceso se realiza a trav√©s de tres etapas:

1. **Extracci√≥n**: Se obtienen datos de cotizaciones cambiarias utilizando la API del BCRA.
2. **Transformaci√≥n**: Se procesan y transforman los datos extra√≠dos para adecuarlos al esquema de la base de datos.
3. **Carga**: Se cargan los datos transformados en una tabla espec√≠fica de Amazon Redshift.

## Estructura del Proyecto

El proyecto tiene la siguiente estructura de directorios:
```
‚îî‚îÄ‚îÄ ./
    ‚îú‚îÄ‚îÄ .github
    ‚îÇ   ‚îî‚îÄ‚îÄ workflows
    ‚îÇ       ‚îî‚îÄ‚îÄ test.yml
    ‚îú‚îÄ‚îÄ BD
    ‚îÇ   ‚îú‚îÄ‚îÄ Creaci√≥n tablas
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TP.Banco.sql
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TP.Moneda.sql
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TP.Pais_Moneda.sql
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TP.Pais.sql
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TP.Region.sql
    ‚îÇ   ‚îî‚îÄ‚îÄ Datos hist√≥ricos y tablas est√°ticas
    ‚îÇ       ‚îú‚îÄ‚îÄ banco_central.csv
    ‚îÇ       ‚îú‚îÄ‚îÄ Carga hist√≥rica cotizaciones.py
    ‚îÇ       ‚îú‚îÄ‚îÄ moneda.csv
    ‚îÇ       ‚îú‚îÄ‚îÄ pais_moneda.csv
    ‚îÇ       ‚îú‚îÄ‚îÄ pais.csv
    ‚îÇ       ‚îî‚îÄ‚îÄ region.csv
    ‚îú‚îÄ‚îÄ dags
    ‚îÇ   ‚îî‚îÄ‚îÄ cotizaciones_dag.py
    ‚îú‚îÄ‚îÄ plugins
    ‚îÇ   ‚îî‚îÄ‚îÄ cotizaciones_plugin
    ‚îÇ       ‚îú‚îÄ‚îÄ scripts
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ carga.py
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ extraccion.py
    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ transformacion.py
    ‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ tests
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_extract.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_load.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_transform.py
    ‚îú‚îÄ‚îÄ .gitignore
    ‚îú‚îÄ‚îÄ BD.png
    ‚îú‚îÄ‚îÄ Creaci√≥n cotizaciones.py
    ‚îú‚îÄ‚îÄ Dash cotizaciones.py   
    ‚îú‚îÄ‚îÄ docker-compose.yaml
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ pytest.ini
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îî‚îÄ‚îÄ run_tests.py

```

## Base de Datos en Redshift

![alt text](BD.png)

**Fact table:**
- **cotizaciones**: contiene la evoluci√≥n diaria de las cotizaciones de diferentes monedas respecto al peso argentino (disponibles en la API del BCRA).

**Dimensiones:**
- **region**
- **moneda**
- **pais**
- **pais_moneda**
- **banco_central**
  

## Instalaci√≥n y configuraci√≥n

#### 1.  Requisitos previos
- Tener instalado:
    * Python
    * Git
    * Docker
- Usuario de AWS Redshift

#### 2. Clonar repositorio

```bash
git clone https://github.com/leomiliano07/Cotizaciones-BCRA.git
cd Cotizaciones-BCRA  
```

#### 3. Configurar variables de entorno
Cargarlas en un archivo `.env`:

```bash
# UID AIRFLOW
AIRFLOW_UID=50000

# Redshift 
REDSHIFT_HOST=
REDSHIFT_PORT=
REDSHIFT_DATABASE=
REDSHIFT_USER=
REDSHIFT_PASSWORD=
REDSHIFT_SCHEMA=

#Paht
PYTHONPATH=
```
#### 4. Crear tabla cotizaciones:

A trav√©s del script Creaci√≥n cotizaciones.py se crea la tabla cotizaciones (fact table) en Redshift, la cual recibir√° la ingesta de la API.

Opcionalmente en el directorio BD se encuentran los csv, query y script para realizar la creaci√≥n y carga del resto de las tablas dimensionales como tambi√©n la carga historica de cotizaciones desde la API.

#### 5. Inicia los contenedores de Docker:

```bash
docker-compose up -d
```
#### 6. Acceder a Airflow UI

Link:
http://localhost:8080

```bash
usuario: airflow 
contrase√±a: airflow
```


#### 7. Modificar Connections en Airflow UI y ejecutar DAG

Ingresar en el UI a Admin -> Connections, agregar una nueva y completar los siguientes campos:
```bash
Connection Id *: Redshift
Connection Id *: Amazon Redshift
Host:
Database:
User:
Password:
Port:

(Estos √∫ltimos campos se deben completar con las mismas credenciales que en el archivo .env)
```
Guardar y luego despausar y ejecutar el DAG.

## Descripci√≥n Scripts ETL

- `extraccion.py`: se encarga de extraer datos de cotizaciones desde la API. Inicia configurando logging para monitorear el proceso. La funci√≥n get_last_loaded_date se conecta a la base de datos Redshift para obtener la √∫ltima fecha de cotizaci√≥n que se ha cargado, lo que ayuda a definir desde qu√© fecha comenzar a extraer nuevos datos. Luego, en extract_data, se establece la fecha de inicio para la extracci√≥n y se hacen solicitudes a la API para cada d√≠a entre esa fecha y el d√≠a actual, guardando los resultados en una lista.
- `transformacion.py`:  transforma los datos que se han extra√≠do de la API. Si hay datos disponibles, recorre cada registro para extraer la fecha y los detalles de las cotizaciones. A continuaci√≥n, crea un DataFrame de pandas que solo incluye las columnas FECHA, C√ìDIGO MONEDA y COTIZACI√ìN. Adem√°s, filtra las filas que contienen ciertos c√≥digos de moneda (ARS, XAG, XAU).
- `carga.py`:  se encarga de cargar un DataFrame de pandas en una tabla de Redshift. Primero, verifica que el DataFrame no sea None y que contenga datos. Si es as√≠, establece una conexi√≥n con la base de datos Redshift y prepara una consulta SQL para insertar los datos en la tabla.
- `cotizaciones_dag.py`:  este DAG gestiona el proceso ETL para las cotizaciones de la API. Las tareas est√°n organizadas en el orden de extracci√≥n, transformaci√≥n y carga, asegurando que cada paso del proceso se realice correctamente. 

## Tests

- `test_extract.py`:  prueba unitaria que simula la extracci√≥n de datos de la API de cotizaciones y verifica que la funci√≥n extract_data retorne una lista con datos correctamente estructurados, incluyendo la cotizaci√≥n esperada.
- `test_transform.py`:  conjunto de pruebas que eval√∫a la funci√≥n transform_data, asegur√°ndose de que los datos se filtren, transformen y estructuren adecuadamente, manejando entradas vac√≠as o incompletas.
- `test_load.py`:  simula la carga de datos a Redshift usando un DataFrame, y mockea la conexi√≥n a la base de datos con psycopg2 para evitar cambios reales, verificando que la funci√≥n load_to_redshift funcione correctamente.

## Dash

Se incluye el script Dash cotizaciones.py, el cual genera un gr√°fico interactivo con la evoluci√≥n diaria de cada moneda, a partir de la informaci√≥n almacenada en la base de datos.

Luego de ejecutar el script, abrir un navegador web y visitar http://127.0.0.1:8050/ para ver el gr√°fico.

Es necesario haber generado y cargado la tabla moneda en Redshift.
